{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 - Check the presence and version of NVidia C Compiler (nvcc)"
      ],
      "metadata": {
        "id": "oUXeJcu-0D_W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkdFacRszgpq",
        "outputId": "63903c97-4402-41d0-c630-ce3c89b0f91c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 - Install the nvcc plugin"
      ],
      "metadata": {
        "id": "UIUMT4L90OnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pe5RJdVzyFv",
        "outputId": "5cea42e0-f642-4fd0-e4ce-bd0d2cb3cb37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-90uph1_z\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-90uph1_z\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4295 sha256=2e363c4ef2fa4ff413d0422216c20ffed57dbbb102d8f689c7db9b1f31234feb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ot1un4ut/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 - Load the nvcc plugin"
      ],
      "metadata": {
        "id": "Psore5uM0YiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMBRsq44z3iR",
        "outputId": "ff882545-609a-41e2-e93c-47e38a32fadd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4 - Attach Google Drive to save the text file with results"
      ],
      "metadata": {
        "id": "b8rlBffK0c8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKQS_aC5JSHG",
        "outputId": "45b7068f-ad63-4978-9410-6ac2a9dbc08b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5 - Run CUDA code"
      ],
      "metadata": {
        "id": "S8pvFWZM0j7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <omp.h>\n",
        "#include <time.h>\n",
        "#include <math.h>\n",
        "\n",
        "#define N 10000\n",
        "#define BLOCK_SIZE 16\n",
        "#define WIDTH 100\n",
        "\n",
        "__global__ void gpu_square_matrix_mult(int *d_a, int *d_b, int *d_c, int n)\n",
        "{\n",
        "\t__shared__ float tile_a[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\t__shared__ float tile_b[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\n",
        "\tint tx = threadIdx.x; int ty = threadIdx.y;\n",
        "\tint bx = blockIdx.x; int by = blockIdx.y;\n",
        "\n",
        "\t// coordinates of specific block\n",
        "\tint row = by * BLOCK_SIZE + ty;\n",
        "\tint column = bx * BLOCK_SIZE + tx;\n",
        "\n",
        "\tint tmp = 0, idx;\n",
        "\n",
        "\tfor(int i=0;i<ceilf(n/(float)BLOCK_SIZE);i++){\n",
        "\t\t// load tile a from Mat a\n",
        "    idx = row*n + i*BLOCK_SIZE + tx;\n",
        "\t\tif(row < n && (i*BLOCK_SIZE + tx)<n)\n",
        "\t\t\ttile_a[ty][tx] = d_a[idx];\n",
        "\t\telse\n",
        "\t\t\ttile_a[ty][tx] = 0;\n",
        "\n",
        "    // load tile b from Mat b\n",
        "    idx = (i*BLOCK_SIZE + ty)*n + column;\n",
        "\t\tif(column < n && (i*BLOCK_SIZE + ty)<n)\n",
        "\t\t\ttile_b[ty][tx] = d_b[idx];\n",
        "\t\telse\n",
        "\t\t\ttile_b[ty][tx] = 0;\n",
        "\n",
        "\t\t// after the entire tile's values are available, proceed\n",
        "\t\t__syncthreads();\n",
        "\n",
        "\t\tfor(int j=0;j<BLOCK_SIZE;j++)\n",
        "\t\t\ttmp += tile_a[ty][j] * tile_b[j][tx];\n",
        "\n",
        "\t\t// after the entire tile's values have been used, proceed\n",
        "\t\t__syncthreads();\n",
        "\t}\n",
        "\t// boundary check\n",
        "\tif(row < n && column < n)\n",
        "\t\td_c[row*n+column] = tmp;\n",
        "}\n",
        "\n",
        "void populate(int *arr, int size){\n",
        "    for(int i = 0; i < size; i++){\n",
        "        arr[i] = (i+1) % 1000;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int *a, *b, *c;\n",
        "\n",
        "    // host copies of variables a, b & c\n",
        "    int *d_a, *d_b, *d_c;\n",
        "\n",
        "    // device copies of variables a, b & c\n",
        "    int size = N * sizeof(int);\n",
        "\n",
        "    struct timespec start, end;\n",
        "    double time_taken = 0.0;\n",
        "\n",
        "    // Allocate space for device copies of a, b, c\n",
        "    cudaMalloc((void **)&d_a, size);\n",
        "    cudaMalloc((void **)&d_b, size);\n",
        "    cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "    // Setup input values\n",
        "    a = (int *)malloc(size);\n",
        "    b = (int *)malloc(size);\n",
        "    c = (int *)malloc(size);\n",
        "\n",
        "    clock_gettime(CLOCK_MONOTONIC, &start);\n",
        "\n",
        "    // Initialize input values\n",
        "    populate(a, N);\n",
        "    populate(b, N);\n",
        "\n",
        "    // Initialize result\n",
        "    for(int i=0; i<N; i++) {\n",
        "        c[i] = 0;\n",
        "    }\n",
        "\n",
        "    // Copy inputs to device\n",
        "    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_c, c, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 gridSize, blockSize;\n",
        "    blockSize.x = blockSize.y = BLOCK_SIZE; blockSize.z = 1;\n",
        "    gridSize.x = ceil(WIDTH/(float)blockSize.x);\n",
        "    gridSize.y = ceil(WIDTH/(float)blockSize.y);\n",
        "    gridSize.z = 1;\n",
        "\n",
        "    // Launch matrix multiplication kernel on GPU\n",
        "    gpu_square_matrix_mult<<<gridSize, blockSize>>>(d_a, d_b, d_c, WIDTH);\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaError err = cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    clock_gettime(CLOCK_MONOTONIC, &end);\n",
        "    time_taken += (end.tv_sec - start.tv_sec);\n",
        "    time_taken += (end.tv_nsec - start.tv_nsec) / 1000000.0;\n",
        "    printf(\"Total time taken (milliseconds): %.8f\", time_taken);\n",
        "\n",
        "    if(err!=cudaSuccess) {\n",
        "        printf(\"CUDA error copying to Host: %s\\n\",\n",
        "        cudaGetErrorString(err));\n",
        "    }\n",
        "    else{\n",
        "        FILE *fp;\n",
        "        fp = fopen(\"/content/gdrive/My Drive/output_gpu.txt\", \"w\");\n",
        "        for(int i=0; i<N; i++) {\n",
        "            fprintf(fp, \"%d \", c[i]);\n",
        "            if((i + 1) % WIDTH == 0) fprintf(fp, \"\\n\");\n",
        "        }\n",
        "        fclose(fp);\n",
        "    }\n",
        "\n",
        "\n",
        "    // Cleanup\n",
        "    free(a); free(b); free(c);\n",
        "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwH6bQ8bXLxA",
        "outputId": "c4c4a754-e268-418b-d9c1-1efff0ed4a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time taken (milliseconds): 0.29757800\n"
          ]
        }
      ]
    }
  ]
}